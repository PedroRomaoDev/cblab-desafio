# Desafio de Engenharia de Dados CBLab

Este reposit√≥rio cont√©m a solu√ß√£o proposta para os desafios 1 e 2 de Engenharia de Dados J√∫nior da **CBLab**.  
O projeto simula um pipeline de dados completo, desde a ingest√£o de APIs at√© o processamento e a disponibiliza√ß√£o para consumo, utilizando uma arquitetura em camadas com **Node.js/Express**, **Docker**, **Node-RED** e ferramentas de qualidade de c√≥digo.

---

## 1. Vis√£o Geral do Projeto

O projeto aborda os desafios de modelagem e ingest√£o de dados em um cen√°rio de uma rede de restaurantes.  
O objetivo √© construir um pipeline de dados funcional que:

- Coleta informa√ß√µes de diversas APIs;
- Armazena os dados em um _data lake_ improvisado (Raw Zone);
- Processa e transforma os dados para um formato estruturado (Processed Zone);
- Disponibiliza os dados para consulta por meio de uma **API de consumo**.

O foco principal √© construir um pipeline **robusto, modular e escal√°vel**, seguindo as melhores pr√°ticas da Engenharia de Dados.

## 2. Estrutura do Reposit√≥rio

A organiza√ß√£o dos arquivos no diret√≥rio raiz (`cblab-desafio/`) segue uma estrutura modular e clara:

```
cblab-desafio/
‚îú‚îÄ‚îÄ docs/                        # Documenta√ß√£o complementar do projeto
‚îÇ   ‚îú‚îÄ‚îÄ data-dictionary.csv     # Dicion√°rio de dados em formato CSV Desafio 1
‚îÇ   ‚îú‚îÄ‚îÄ desafio01.png           # Diagrama de modelagem l√≥gica do Desafio 1
‚îÇ   ‚îú‚îÄ‚îÄ desafio01.sql          # Transcri√ß√£o SQL do modelo de dados do Desafio 1
‚îÇ   ‚îî‚îÄ‚îÄ swagger.json            # Documenta√ß√£o (Swagger) da API
‚îú‚îÄ‚îÄ nodered_data/               # Volume persistente para dados e fluxos do Node-RED
‚îÇ   ‚îú‚îÄ‚îÄ raw/                    # Zona Bruta do Data Lake (JSONs brutos)
‚îÇ   ‚îî‚îÄ‚îÄ processed/              # Zona Processada do Data Lake (JSONs transformados)
‚îú‚îÄ‚îÄ src/                        # C√≥digo-fonte da aplica√ß√£o Express
‚îÇ   ‚îú‚îÄ‚îÄ controllers/            # Camada de apresenta√ß√£o (lida com requisi√ß√µes HTTP)
‚îÇ   ‚îú‚îÄ‚îÄ factories/              # Fun√ß√µes de f√°brica para criar objetos completos (Factory Pattern)
‚îÇ   ‚îú‚îÄ‚îÄ helpers/                # Fun√ß√µes auxiliares (ex: respostas HTTP padronizadas)
‚îÇ   ‚îú‚îÄ‚îÄ repositories/           # Camada de acesso a dados (lida com leitura/escrita de arquivos)
‚îÇ   ‚îî‚îÄ‚îÄ usecases/               # Camada de l√≥gica de neg√≥cio
‚îú‚îÄ‚îÄ tests/                      # Testes unit√°rios (Jest)
‚îú‚îÄ‚îÄ api-server.js               # Ponto de entrada principal da API Express 
‚îú‚îÄ‚îÄ docker-compose.yml          # Defini√ß√£o e orquestra√ß√£o dos servi√ßos Docker
‚îú‚îÄ‚îÄ Dockerfile                  # Instru√ß√µes para construir a imagem Docker do api-server
‚îî‚îÄ‚îÄ nodered.Dockerfile          # Instru√ß√µes para construir a imagem Docker do Node-RED
```

## 3. Contexto para os Desafios

### Desafio 1 - Modelagem de Esquema e Transcri√ß√£o para SQL

**Contexto:**  
Analisar o esquema de um JSON de um ERP de restaurante e modelar um banco de dados relacional. √â necess√°rio garantir que as tabelas representem entidades reais e acomodem a natureza polim√≥rfica dos dados (`detailLines`).

**Solu√ß√£o:**  
O esquema do `ERP.json` foi analisado e modelado em um banco de dados relacional com 10 tabelas, incluindo entidades mestras como `stores` e `employees`. A modelagem utiliza normaliza√ß√£o e chaves prim√°rias/estrangeiras para garantir a integridade dos dados e um design robusto. A estrutura polim√≥rfica dos `detailLines` (discount, serviceCharge, tenderMedia e errorCode) foi resolvida com sucesso atrav√©s de tabelas de subtipo com relacionamentos 1:1.

- üìÑ **[Dicion√°rio de Dados (CSV)](docs/data-dictionary.csv)**: Documenta detalhadamente cada campo do JSON.
- üßæ **[Transcri√ß√£o SQL](docs/desafio01.sql)**: C√≥digo compat√≠vel com MySQL.
- üñºÔ∏è **[Diagrama de Modelagem](docs/desafio01.png)**: Representa√ß√£o l√≥gica do modelo criado.

## 1. Como a Modelagem Foi Realizada e Por Qu√™

A abordagem principal adotada foi a **normaliza√ß√£o de dados**, um pilar da modelagem relacional. Em vez de criar uma √∫nica tabela monol√≠tica para cada `guestChecks` (que seria uma representa√ß√£o direta do JSON, mas altamente ineficiente), as entidades l√≥gicas foram separadas em tabelas distintas.

### Entidades Principais

A tabela `guestChecks` foi criada como a entidade central para armazenar os dados de pedido mais gen√©ricos (IDs, totais, datas, informa√ß√µes de funcion√°rio e mesa).

### Tabelas de Refer√™ncia

As tabelas `stores` e `employees` foram adicionadas acima da tabela `guestChecks`. Esta foi uma decis√£o estrat√©gica para aprimorar a modelagem, pois em uma rede de restaurantes, as lojas e os funcion√°rios s√£o entidades mestras com informa√ß√µes pr√≥prias que n√£o devem ser duplicadas em cada pedido. Isso reflete um modelo de neg√≥cio real.

### Relacionamentos

As tabelas s√£o conectadas por chaves prim√°rias e estrangeiras. Por exemplo, `guestChecks.locRef` e `guestChecks.empNum` s√£o chaves estrangeiras que referenciam as chaves prim√°rias (`locRef` e `empNum`) nas tabelas `stores` e `employees`, respectivamente.

Essa separa√ß√£o garante a **integridade dos dados**, evita a **redund√¢ncia** e melhora o **desempenho das consultas**.

---

## 2. Como Lidar com os Arrays (`taxes`, `detailLines`)

A modelagem de arrays de objetos em um esquema relacional exige uma abordagem cuidadosa, e o m√©todo escolhido foi a cria√ß√£o de **tabelas de relacionamento 1:N (um para muitos)**.

### Array `taxes`

O array de impostos em um pedido foi modelado como uma tabela separada: `guestCheck_Taxes`. Cada registro nesta tabela representa um imposto espec√≠fico aplicado a um pedido.

- A chave estrangeira `guestCheckId` na tabela `guestCheck_Taxes` referencia a chave prim√°ria `guestCheckId` na tabela `guestChecks`.
- Isso reflete a cardinalidade 1:N: um pedido pode ter muitos impostos, mas cada imposto pertence a um √∫nico pedido.

### Array `detailLines` (modelo polim√≥rfico)

#### Tabela Central

Foi criada uma tabela central `guestCheck_DetailLines` para armazenar os campos comuns a todos os tipos de itens de linha. Esta tabela se relaciona com `guestChecks` em uma rela√ß√£o 1:N.

#### Tabelas de Subtipo (1:1)

Para cada tipo de item que pode estar em `detailLines`, uma tabela separada foi criada (`menuItem`, `discount`, `serviceCharge`, `tenderMedia` e `errorCode`, etc.).

- A chave prim√°ria (`guestCheckLineItemId`) da tabela `guestCheck_DetailLines` √© usada como chave prim√°ria e estrangeira nas tabelas de subtipo.
- Isso cria um relacionamento de **um para um (1:1)** entre `guestCheck_DetailLines` e cada uma das tabelas de subtipo.
- Garante que um item de linha seja de **um e apenas um tipo**.

---

## 3. Justificativa das Escolhas com Base em Opera√ß√µes de Restaurante

- **Controle de Pedidos e Auditoria**: A separa√ß√£o das tabelas permite uma auditoria clara de cada pedido ‚Äî subtotal, impostos aplicados, descontos e itens vendidos.

- **C√°lculo de Impostos e Pre√ßos**: A tabela `guestCheck_Taxes` permite que a equipe de BI calcule a arrecada√ß√£o de impostos por pedido, por tipo de imposto e por loja.

- **An√°lise de Vendas**: A tabela `menuItem` permite an√°lises detalhadas, como ‚Äúqual item foi mais vendido‚Äù ou ‚Äúqual faixa de pre√ßo √© mais popular‚Äù.

- **Performance e Escalabilidade**: A normaliza√ß√£o e o particionamento impl√≠cito por entidades (`stores`, `employees`) tornam o banco mais eficiente em larga escala.

- **Qualidade de Produ√ß√£o**: A modelagem segue boas pr√°ticas exigidas por sistemas ERP que demandam alta **integridade referencial** ‚Äî essencial em ambientes de produ√ß√£o.

---

## 4. Desafio 2 - Datalake e Pipeline de Ingest√£o e Processamento

### Contexto:

Consumir 5 endpoints de API de uma rede de restaurante e justificar a import√¢ncia de por que guardar as respostas da API e como ir√≠amos organizar as respostas dentro do data lake da empresa, levando em considera√ß√£o poss√≠veis mudan√ßas futuras e manter a velocidade das opera√ß√µes de manipula√ß√£o, verifica√ß√£o, busca e pesquisa.

### Solu√ß√£o:

Foi constru√≠do um pipeline de dados funcional e conteinerizado, orquestrado pelo **Node-RED**. A arquitetura da solu√ß√£o foi projetada com base nos padr√µes do mercado, seguindo o modelo de arquitetura de **data lake em zonas**.

- **Raw Zone (Dados Brutos):**  
  A primeira camada do data lake armazena as respostas das APIs no formato original (`.json`), sem modifica√ß√µes, para garantir a **fidelidade** e a **rastreabilidade** dos dados.

- **Processed Zone (Dados Curados):**  
  A segunda camada armazena os dados que foram **processados, transformados e padronizados**. Nesta etapa, a l√≥gica de neg√≥cio lida com a **evolu√ß√£o de esquema**, como a renomea√ß√£o do campo `taxes` (presente na Raw Zone) para `taxation`.

> üóÇÔ∏è A estrutura de armazenamento dos arquivos √© **particionada hierarquicamente** por `apiName`, `busDt` (data de neg√≥cio) e `storeId`, otimizando opera√ß√µes de **manipula√ß√£o, verifica√ß√£o, busca e pesquisa**.
> ![Exemplo da Estrutura](assets/raw.png)

### Justificativa:

- **üîç Rastreabilidade e Auditoria:**  
  O armazenamento dos dados brutos na Raw Zone √© crucial para **auditar o pipeline** e permitir o **reprocessamento** dos dados em caso de falhas ou mudan√ßas na l√≥gica de neg√≥cio.

- **üß¨ Flexibilidade de Esquema (Schema-on-Read):**  
  A arquitetura do data lake permite a ingest√£o de dados com **esquemas vari√°veis**, com o `ProcessDataUseCase` sendo respons√°vel por impor o esquema correto apenas na camada de dados curados (**Processed Zone**).  
  Isso torna a solu√ß√£o **resiliente** a futuras mudan√ßas nas APIs de origem.

- **‚ö° Performance:**  
  O particionamento dos dados por `apiName`, `busDt` e `storeId` garante que ferramentas de consulta (em um cen√°rio real, como **Apache Spark** ou **Amazon Athena**) possam escanear apenas o subconjunto de dados necess√°rio, melhorando **drasticamente o desempenho das consultas**.

---

## 5. Configura√ß√£o do Ambiente

### 5.1. Pr√©-requisitos:

- Ter o **Docker Desktop** instalado e rodando.
- Ter um terminal (ex: **PowerShell** ou **Git Bash**) na pasta raiz do projeto.
- Ter uma conta no **GitHub**.

### 5.2. Passos para Rodar o Projeto:

- Clone o Reposit√≥rio:

````bash
git clone https://github.com/PedroRomaoDev/cblab-desafio
cd cblab-desafio

### Inicie os Cont√™ineres

Este comando constr√≥i as imagens personalizadas e inicia os servi√ßos em segundo plano:

```bash
docker-compose build

docker compose up -d
````

### Acesse as Interfaces

- **Node-RED:** [http://localhost:1880](http://localhost:1880)
- **API Express (Swagger UI):** [http://localhost:3001/docs](http://localhost:3001/docs)

### Popule o Data Lake

1. No Node-RED, v√° direto ao canto superior direito e fa√ßa o import dos fluxos, presentes no arquivo [meus_fluxos_nodered_backup.json](./meus_fluxos_nodered_backup.json).
2. Ap√≥s realizar a importa√ß√£o, clique em n√≥ **implementar** para disparar a ingest√£o de dados mock.
3. A pasta raw ser√° criada dentro do volume do container, contendo os dados brutos.
4. Para visualizar os dados brutos, voc√™ pode acessar a pasta nodered_data/raw.

## Processe os Dados (via API)

A segunda camada (**Processed Zone**) √© populada atrav√©s da API de Processamento.

1. V√° para a interface do Swagger UI em [http://localhost:3001/docs](http://localhost:3001/docs).
2. Na se√ß√£o **"Processamento de Dados"**, expanda o endpoint `POST /process-data`.
3. Clique em **"Try it out"** e depois em **"Execute"** (com o corpo vazio `{}`).

Isso ir√° ler os dados da **Raw Zone**, transform√°-los e salv√°-los na pasta `nodered_data/processed/`.

## 5.3. Testes Unit√°rios

Execute os testes:

```bash
npm test
```

**Resultados:** Voc√™ deve ver que todos os testes para os reposit√≥rios, use cases e controllers foram executados e passaram com sucesso.

## Rotas das APIs

### GET /raw-zone (Dados Brutos)

### POST /process-data (Disparar Processamento)

### POST /query/{apiName} (Consulta de Dados Processados)

### POST /item-lookup (Consulta de Item por ID)

## 6. Considera√ß√µes Finais e Melhorias

A arquitetura em camadas e a inje√ß√£o de depend√™ncia foram implementadas para garantir a qualidade de produ√ß√£o, modularidade e testabilidade do c√≥digo.

O uso de Docker Compose e Node-RED garante um ambiente de desenvolvimento consistente e um pipeline de dados visualmente represent√°vel.

### Melhorias Futuras

- Conectar o data lake a um provedor em nuvem (Amazon S3 ou Google Cloud Storage) para maior escalabilidade.
- Adicionar monitoramento e alertas para falhas no pipeline de ingest√£o/processamento.
- Criar um cat√°logo de dados para facilitar a descoberta e o uso dos dados.
- Implementar autentica√ß√£o e autoriza√ß√£o para as APIs.

### Quadro Kanban do Projeto

Acompanhe o progresso do desenvolvimento e as tarefas finalizadas neste link:  
[Quadro Kanban](https://github.com/users/PedroRomaoDev/projects/1/views/1?layout=board)

---

Este documento √© uma parte da entrega do desafio, complementando o c√≥digo e as instru√ß√µes.
